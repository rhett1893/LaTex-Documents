\documentclass[12pt]{article}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0in}
\setlength{\parskip}{\baselineskip}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{float}
\pagestyle{fancy}


\begin{document}

\lhead{{\bf CSCI 3104: Algorithms \\ Problem Set 6 (70 points)} }
\rhead{{\bf Rhett\ Hanscom\\ Summer 2019, CU-Boulder}}
\renewcommand{\headrulewidth}{0.4pt}

\vspace{-3mm}
\begin{enumerate}
    % --- EASY PROBLEM
    \item (15 points) Give an $O(VE)$-time algorithm for computing the transitive closure of a directed graph $G=(V,E)$.  Compute its asymptotic running time.
    
    \pagebreak
\textbf{Solution to Problem 1:}

\begin{enumerate}
    \item When creating a matrix to hold the transitive closure of a graph data, looking for a path between two vertices, we would, at worst, have to traverse the entire graph, searching through and either DFS or BFS method with each node as the root, for each node traveled to from here, mark the position in the matrix [$V_{root}$, $V_{found}$] with a 1. The time complexity of traversing the entire graph with each node used as the root would have a complexity of \textit{O(VE)}.
\end{enumerate}
\pagebreak

    % --- MEDIUM PROBLEM
	\item (15 points) Grog --master of pictures-- needs your help to compute the in- and out-degrees of all vertices in a directed multigraph $G$. However, he is not sure how to represent the graph so that the calculation is most efficient. For each of the three possible representations, express your answers in asymptotic notation (the only notation Grog understands), in terms of $V$ and $E$, and justify your claim.
	\begin{enumerate}
	\item An {\em adjacency matrix} representation. Assume the size of the matrix is known.
	\item An {\em edge list} representation. Assume vertices have arbitrary labels.
	\item An {\em adjacency list} representation. Assume the vector's length is known.
	\end{enumerate}
	
	\pagebreak
\textbf{Solution to Problem 2:}

\begin{enumerate}
    \item For the \textit{adjacency matrix}, the most efficient representation in terms of \textit{V} and \textit{E} would have a space complexity of O($V^{2}$) as the matrix storing the directed edges would contain \textit{V} rows and \textit{V} columns. The time complexity to create an \textit{adjacency matrix} would be O(V+E), the time it would take to traverse the graph in a DFS or BFS manner, marking edges in the matrix during the traversal. 
    In order to compute the in-degree for each vertex, simply scan the V\textit{th} row, and in order to compute the out-degree scan the V\textit{th} column. This would result in a time complexity of O($V^{2}$) a for all nodes, and \textit{O(V)} for a single node.
    
    \item For the \textit{edge list}, the most efficient representation in terms of \textit{V} and \textit{E} would have a space complexity of O(\textit{E}), where there are \textit{E} ordered pairs, each representing an edge between ($V_{out}$, $V_{in}$). The time complexity to create an \textit{edge list} would be O(V+E), the time it would take to traverse the graph in a DFS or BFS manner, creating the the lists of edges as they were discovered.
    
    To calculate the out-degree for a particular node \textit{V}, traverse the \textit{V}-pairs, counting the number of times \textit{V} appears in the \textit{$V_{out}$} position. This would be a complexity of O(\textit{E}) for a single vertex and O(\textit{VE}) for all nodes within the graph.
    
    \item For the \textit{adjacency list}, the most efficient representation in terms of \textit{V} and \textit{E} would have a space complexity of O(\textit{V}+\textit{E}), where there would be \textit{V} lists with \textit{E} entries. The time complexity to create an \textit{adjacency list} would be O(V+E), the time it would take to traverse the graph in a DFS or BFS manner, creating the the lists of edges as they were discovered.
    
    The \textit{V} list would show the out-degree edges for each Vertex. The time complexity to compute the out degree edges is O(\textit{V}) for a single vertex and O(\textit{V}+\textit{E}) for an the entire graph. 
    
    The in-degrees would need to be counting the number of times a specific \textit{V} appears in each of the \textit{V} lists. The time complexity to compute this would be O(\textit{V}+\textit{E}) for a single vertex and O(\textit{VE}) to complete the in- and out- degrees for each node in the graph.
\end{enumerate}
\pagebreak	

    % --- MEDIUM PROBLEM
    \item (40 points) Consider a valleyed array $A[1, 2, \ldots, n]$ with the property that the subarray $A[1\ldots i]$ has the property that $A[j] > A[j + 1]$ for $1 \leq j < i$, and the subarray $A[i \ldots n]$ has the property that $A[j] < A[j + 1]$ for $i \leq j < n$. For example, \newline $A = [16, 15, 10, 9, 7, 3, 6, 8, 17, 23]$ is a valleyed array.
    
    \begin{enumerate}
        \item Write a recursive algorithm that takes asymptotically sub-linear time to find the minimum element of $A$.
        \item Prove that your algorithm is correct. (Hint: prove that your algorithm's correctness follows from the correctness of another correct algorithm we already know.)
        \item Now consider the multi-valleyed generalization, in which the array contains $k$ valleys, i.e., it contains $k$ subarrays, each of which is itself a valleyed array. Let $k = 2$ and prove that your algorithm can fail on such an input.
        \item Suppose that $k = 2$ and we can guarantee that neither valley is closer than $n=4$ positions to the middle of the array, and that the "joining point" of the two singly valleyed subarrays lays in the middle half of the array. Now write an algorithm that returns the minimum element of $A$ in sublinear time. Prove that your algorithm is correct, give a recurrence relation for its running time, and solve for its asymptotic behavior.
    \end{enumerate}
 \pagebreak
 \textbf{Solution to Problem 3:}

\begin{enumerate}
    (Assume zero indexing for this entire solution) \newline
    \item 
    A recursive algorithm which takes asymptotically sub-linear time to find the minimum element of A, when given A, an array, f, the first element's index in the array, and l, the last element's index:
    \begin{verbatim}
    valleyMin(A, f, l):
        middle = ceiling((f + l)/2)
        if f == l
            return A[f]
        elif A[middle + 1] > A[middle] and A[middle - 1] > A[middle]
            return A[Middle]
        elif (A[middle + 1] < A[middle])
            return valleyMid(A, middle, l)
        else
            return valleyMin(A, s, middle)
    \end{verbatim}
    This algorithm will run in logarithmic time, which is sub-linear.
    
    \item
    
    We can prove this algorithm with a proof by contradiction. Let us assume that this algorithm does not return the minimum element in a subarray, which runs from A[f] to A[l]. There are four possible outcomes to out recursive algorithm after the middle of the array is established.
    In the first case, where the index of the first element is equal to the index of the last element, we have trivially found the minimum element and return in. This is a contradiction to our proof, and therein is correct.
    In the second possible outcome, we return a value which is less than each of the elements which come directly before and after it. We return this value as the min. We know this MUST be the minimum, as contracting this would defy the definition of a valleyed array. Thus, by contradiction we have proven this case to correctly handle a valleyed minimum.
    In the third possible case, the element of the subarray which is one greater than the middle index is less than the element at the middle index. In this case, we recurse on A from the middle element to the last element,as by the definition of a valleyed array we know that the minimum must have an index greater than or equal two the middle index we are examining. We know this by the definition of a valleyed array.
    In the last case, we have either already determined that we have a minimum value, and return that value, or we have established that the minimum must be in the part of the array which begins at the middle index and spans through the last index, or we are forced to consider the elements we have not yet considered. As such, we recurse on A from the first element to the middle element. This is correct. 
    
    Therefore, we have shown by a proof by contradiction that when we refund a minimum value, it will be the correct value.
    \item
    Consider the array A = [6, 5, 4, 3, 2, 1, 5, 10, 0, 5], where there are $k$ = 2 valleyed sub-arrays ([6, 5, 4, 3, 2, 1, 5] and [10, 0, 5, 12]), with an overall minimum element of A[8] = 0. With a proof by counter example, we can see that the algorithm from part $a$ of this solution will not obtain the correct element as the minimum in the array. 
    
    Running through the psuedo code, the passed first index would be 0 and last would be 9. This would result in the middle index of 5 (the ceiling of 4.5). When comparing (A[6] \textgreater A[5] and A[4] \textgreater A[5]), the Boolean for this would be true and this branch would execute, returning the value A[5] = 1 as the minimum value, though we know it to be A[8] = 0. Therefore we have shown with a counter example that our algorithm will NOT necessarily work for arrays with multiple valleyed subarrays.
    
    \newpage
    \item
    Suppse we know there are $k$ = 2 valleyed subarrays, and that neither valley is closer than $n$ = 4 to the middle of the array, and that the joining point of these two subarrays is the middle element of the combined array. The algorithm for identifying a minimum in such an array is as follows:
     

    \begin{verbatim}
    valleyMin(A, f, l):
        if f == l
            return A[f]
        middle = ceiling((f + l)/2)
        if A[middle + 1] > A[middle] and A[middle - 1] > A[middle]
            return A[middle]
        elif (A[middle + 1] < A[middle])
            return valleyMid(A, middle, l)
        else
            return valleyMin(A, s, middle)
    
    valleyMax(A, f, l):
        middle = ceiling((f + l)/2)
        if f == l
            return f
        if A[middle + 1] < A[middle] and A[middle - 1] < A[middle]
            return middle
        elif (A[middle + 1] > A[middle])
            return valleyMid(A, middle, l)
        else
            return valleyMin(A, s, middle)
    
    twoValleyedMin(A):
        middle = valleyMax(A.length() - 4, A.length() + 4)
        min1 = valleyMin(A, 0, middle)
        min2 = valleyMin(A, middle + 1, A.length() - 1)
        if min1 < min2
            return min1
        else
            return min2
    \end{verbatim}
    
    In our solution we build on what was previously established in parts $a$ and $b$ respectively and will use a proof by contradiction strategy to show that these algorithms behave correctly and as expected. 
    
    To prove the correctness of this algorithm, let us first assume that the proof of $valleyMin$ from part $b$ holds true. Let us also assume that $twoValleyMin$ will NOT correctly return of the overall min of an array with two valleys, as described in the problem set.
    
    The problem states that the joining of each of the two valleyed arrays occurs within 4 indexes on either side of the middle index. 
    
    We will use our function $maxValley$ function to find the joining index of the two subarrays. 
    
    We can use a proof by counter example to demonstrate that this is correct. Let us assume that $maxValley$ returns an element which is not the maximum of a subarray.
    
    We are told that the joining point will be within 4 indices of the middle index. Therefore, we pass $maxValley$ a subarray which contains the elements from middle index - 4 to middle index + 4. $maxValley$  then computes the middle index of this subarray and checks to see if it is a maximum, we will find this value trivially if the index of the first element we are examining is the same as the last index we are examining. In this case, we return this index, contradicting our assumption that we will not return the index of the maximum value in the subarray. 
    
    If a maximum has been found, we return this index to be used as the joining index within our $twoValleyMin$ function. This is correct, and is a contradiction to our assumption for this proof. 
    
    If the middle index is less than the next greatest index, we recurse on A from the middle index index to the last index as we know the max must be found after this middle element, which we have established is less than the maximum value of the subarray. If we have not found the maximum element outright, and we have also found that the middle element is greater than the following element, we know the maximum element must have occurred before the middle element we are examining and therefore recurse on A from the first index to the middle index. This is correct and will correctly identify the maximum element in a subarray, to then be used as the 'joining element'.
    
    
    We divide the array into elements from the first position to the joining element and also from the element directly following the joining element all the way to the final element and then call $valleyMin$ on each of these subarrays. We can safely disregard the exactly joining element as we know that each subarray will have a valley floor at least 4 positions away from the middle element and it would therefore be a local maximum and NOT eligible as a minimum. As we have assumed that $valleyMin$ successfully identifies the minimum element in an array with a single valley, we will produce the minimum element from each subarray.
    
    We then compare the two minimum elements and return the smaller of the two values. 
    
    We know that the returned value \textbf{must} be the minimum element of the overall array A, which contradictions our initial assumption that this algorithm will not return the correct minimum element of array A.
    
    Therefore we have proven that $twoValleyMin$ successfully identifies the overall minimum element in an array with $k$ = 2 valleyed subarrays. 
    
    A recurrence relation for the running time of this algorithm is given as:
    \newline
    T(n) = 2T(n/2) + 2T(n/2) + 2T(n/2) + c \newline
    . \newline
    . \newline
    T(n) = 6T(n/2) + c

    \newline
    Now, solving for T(n): \newline
    T(n) = 6 * $\theta$(log(n/2)) + 2 \newline
    T(n) = $\theta(logn)$
    
    
    
\end{enumerate}
\pagebreak
\end{enumerate}
\end{document}


